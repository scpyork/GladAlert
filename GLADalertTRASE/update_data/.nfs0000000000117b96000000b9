#!/usr/bin/env python
'''
http://glad-forest-alert.appspot.com/

File hierarchy:

latlon (minx|maxx|miny|maxy)
    -> year (yyyy)
        -> dataset (pixelmat)
           ->> dataset attrs


'''

year = False;

from .new_alerts import *
import numpy as np
import re,os,h5py,tempfile,datetime,gc,sys
from PIL import Image # $ pip install pillow
Image.MAX_IMAGE_PIXELS = None

#create the directoryn
if not os.path.isdir('./data/'):os.mkdir('./data/')
tempdir = tempfile.gettempdir()


now = datetime.datetime.now()
if not year: year = str(now.year)

## Create the wrapper hdf5 file
with h5py.File('glad_data.h5','a') as hf:
    try:      hf.attrs['created_on']
    except:   hf.attrs['created_on'] = str(now)
    latlon = hf.keys()

    files = getdates(int(year))
    print ('starting')


    files_year = filter(lambda x: re.match(r'.*/alert'+year[-2:]+'_',x), files)

    daymonths = list(set([i.split('/')[-2] for i in files_year]))


    select = daymonths[0]

    filtered = filter(lambda x: select in x, files_year)
    print len(filtered)

    for keep in filtered:
    # filter out just the latest year, the previous exists in a separate directory
        name = keep.split('/')[-1]
        area = re.findall(r'_(\d+[NESW\b])',name)
        # current file position
        position = map(direction, area)
        group = '|'.join((str(i) for i in position))

        ## create test cascade
        if group in latlon : group = hf[group]
        else :
            latlon.append(group)
            group = hf.create_group(group)
            # add this on such that we dont get problems when moving on to new countries or years...

        #year
        if year in group.keys():g_year = group[year]
        else:g_year = group.create_group(year)

        date = keep.split('/')[-2]
        url = keep.replace('gs://','https://storage.cloud.google.com/')#+'?authuser=0'
#2
        print (os.popen('gsutil cp %s %s/%s'%(keep,tempdir,name)).read())

        im = Image.open('%s/%s'%(tempdir,name))
        data = np.array(im,int)
        os.system('rm %s/*.tif'%tempdir)

        err = 0
        while len(data.shape) == 0:
            gc.collect();
            im = Image.open('%s/%s'%(tempdir,name))
            data = np.array(im)
            print ('memory issues - ',name)

            err+= 1
            if err > 20:
                hf.close()
                sys.exit('failing to open image')



        print data.shape

        # dont write again
        if date in g_year.keys(): continue

        dset = g_year.create_dataset(date, data.shape , dtype='int8', data = data,compression='gzip',compression_opts=9)

        dset.attrs['min_x']= position[0]
        dset.attrs['min_y']= position[1]
        dset.attrs['max_x']= position[2]
        dset.attrs['max_y']= position[3]

        dset.attrs['url']=url
        dset.attrs['shape']= data.shape
        group.attrs['shape']= data.shape


        print(group,date)

    hf.attrs['last_modified'] = str(datetime.datetime.now())
#hf.close()
os.system('rm -rf %s/*.tif'%tempdir)
print( 'finished ')
